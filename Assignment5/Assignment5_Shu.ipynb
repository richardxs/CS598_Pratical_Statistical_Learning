{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team member and Contributions\n",
    "> Shu Xu (shuxu3@illinois.edu)\n",
    "\n",
    "> Yan Han (yanhan4@illinois.edu)\n",
    "\n",
    "> Amrit Kumar(amritk2@illinois.edu)\n",
    "\n",
    "**We finish this notebook together.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this assignment, you will be implementing a linear Support Vector Machine (SVM) classifier from scratch using stochastic gradient descent (SGD).\n",
    "\n",
    "As we discussed in class, traditional SVMs often solve the dual problem, which involves a quadratic objective function subject to linear constraints. While this approach can be efficient for small-scale tasks, it becomes less practical for large-scale problems. In such cases, we can leverage the benefits of SGD to directly solve the primal problem.\n",
    "\n",
    "## The SGD Algorithm\n",
    "The SGD algorithm works as follows:\n",
    "\n",
    "1. Start by choosing a random initial value of parameters\n",
    "2. Loop Over Epochs:\n",
    "    - In each epoch, go through the entire dataset once. An epoch is a complete pass through all the training data.\n",
    "3. Loop Over Data Points:\n",
    "    - Within each epoch, iterate over each data point in your training dataset.\n",
    "4. Update the Gradient:\n",
    "    - For each data point, calculate the gradient of the loss function with respect to the current parameter values. This gradient represents the direction of steepest ascent.\n",
    "5. Calculate Step Sizes:\n",
    "    - For each parameter, calculate the step size as : step size = gradient * learning rate.\n",
    "6. Update Parameters:\n",
    "    - Update new parameters as : new params = old params - step size\n",
    "7. Repeat Until Convergence:\n",
    "    - Repeat steps 3 to 6 for each data point in the dataset. Continue this process for a fixed number of epochs or until convergence criteria are met.\n",
    "\n",
    "\n",
    "## Pegasos Algorithm\n",
    "The Pegasos (Primal Estimated sub-GrAdient SOlver for SVM) algorithm, proposed by Shalev-Shwartz et al. (2011) [Paper Link](https://home.ttic.edu/~nati/Publications/PegasosMPB.pdf), is an application of SGD.\n",
    "\n",
    "Recall that the primal problem of linear SVM can be expressed as the following the Loss + Penalty format:\n",
    "$$\n",
    " \\frac{\\lambda}{2} \\|\\beta\\|^2 + \\frac{1}{n} \\sum_{1}^{n}\\left[ 1 - y_{i}\\left( x_{i}^{t}\\beta + \\alpha \\right) \\right]_{+}\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is the intercept and $\\beta$ is the p-dimensional coefficient vector\n",
    "\n",
    "The **Pegasos Algorithm** can be summarized as follows:\n",
    "1. initialize $\\beta = 0_{p \\times 1}$, $\\alpha_1 = 0$, and $t = 0$\n",
    "2. For *epoch = 1, 2, ..., T* do\n",
    "    for *i = 1, 2, ..., n* do \n",
    "    - $t = t + 1, \\eta_t = \\frac{1}{t \\lambda}$\n",
    "    - update $\\beta_{t+1} \\Leftarrow \\beta_t - \\eta_t \\Delta_t$\n",
    "    - update $\\alpha_{t+1} \\Leftarrow \\alpha_t - \\eta_t \\delta_t$\n",
    "Here $\\eta_t$ is the learning rate, and $\\Delta_t$ and $\\delta_t$ are the (sub)grdient of $J_i\\left(\\beta, \\alpha \\right)$, where $\\beta=\\beta_t$, and $\\alpha = \\alpha_t$:\n",
    "$$\n",
    "    J_i\\left( \\beta, \\alpha \\right) = \\frac{\\lambda}{2} \\|\\beta\\|^2 + \\left[ 1 - y_i\\left( x_{i}^{t}\\beta + \\alpha \\right) \\right]_{+}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\Delta_t = \n",
    "    \\begin{cases}\n",
    "    \\lambda \\beta_t - y_i x_i &  if \\space y_i\\left( x_{i}^{t}\\beta_t + \\alpha_t\\right) < 1\\\\\n",
    "    \\lambda \\beta_t & otherwise \\\\\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\delta_t =\n",
    "    \\begin{cases}\n",
    "    -y_i  &  if \\space y_i\\left( x_{i}^{t}\\beta_t + \\alpha_t\\right) < 1\\\\\n",
    "    0 & otherwise\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "### Implement the **Pegasos Algorithm**\n",
    "\n",
    "\n",
    "- Use a fixed number of epochs, e.g., T = 20.\n",
    "- In each epoch, before going through the dataset, consider randomizing the order of the data points. To achieve this, you should set random seeds for \n",
    "shuffling. For this assignment, the seeds used for shuffling do not need to be associated with your UIN.\n",
    "\n",
    "- Test your code with the provided training (200 samples) and test (600 samples) datasets, which are subsets of the MNIST data. Each dataset consists of 257 columns, with the first 256 columns representing the features, and the last column indicating the label (either 5 or 6).\n",
    "    - [coding5_train.csv](https://liangfgithub.github.io/Data/coding5_train.csv)\n",
    "    - [coding5_test.csv](https://liangfgithub.github.io/Data/coding5_test.csv)\n",
    "\n",
    "- Report **confusion tables** on the **training and test** datasets.\n",
    "\n",
    "- Your code should obtain **less than 15%** test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv(\"coding5_train.csv\")\n",
    "test_data = pd.read_csv(\"coding5_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_X = training_data.drop(columns=[\"Y\"])\n",
    "training_Y = training_data[\"Y\"]\n",
    "training_transformed_Y = training_Y.transform(lambda x: -1 if x==5 else 1)\n",
    "test_X = test_data.drop(columns=[\"Y\"])\n",
    "test_Y = test_data[\"Y\"]\n",
    "test_transformed_Y = test_Y.transform(lambda x: -1 if x==5 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = len(training_X.columns)\n",
    "n_training_samples = len(training_X)\n",
    "n_test_samples = len(test_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 20 # number of epochs\n",
    "beta = np.zeros(n_features) # p-diemnsional coefficient vector\n",
    "alpha = 0.0    # intercept\n",
    "Lambda = 0.001\n",
    "t = 0\n",
    "training_index = np.linspace(0, n_training_samples - 1, n_training_samples).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 7.408633070123595, at Epoch = 1\n",
      "Loss = 0.6128011086573925, at Epoch = 2\n",
      "Loss = 1.0122485668368608, at Epoch = 3\n",
      "Loss = 0.3280991577292658, at Epoch = 4\n",
      "Loss = 1.635310344504758, at Epoch = 5\n",
      "Loss = 0.3654115040676238, at Epoch = 6\n",
      "Loss = 0.12943846237187898, at Epoch = 7\n",
      "Loss = 0.3801420252009566, at Epoch = 8\n",
      "Loss = 0.06715177016867417, at Epoch = 9\n",
      "Loss = 0.24278772439915258, at Epoch = 10\n",
      "Loss = 0.04733629574475289, at Epoch = 11\n",
      "Loss = 0.15754779665111132, at Epoch = 12\n",
      "Loss = 0.34115858659353965, at Epoch = 13\n",
      "Loss = 0.1296421680448115, at Epoch = 14\n",
      "Loss = 0.06483007836792057, at Epoch = 15\n",
      "Loss = 0.41762269696574494, at Epoch = 16\n",
      "Loss = 0.045803347274405534, at Epoch = 17\n",
      "Loss = 0.22003209026875148, at Epoch = 18\n",
      "Loss = 0.04478130371246873, at Epoch = 19\n",
      "Loss = 0.04489556562596132, at Epoch = 20\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, T+1):\n",
    "    np.random.seed(epoch)\n",
    "    np.random.shuffle(training_index)\n",
    "    for i in training_index:\n",
    "        X_i, y_i = training_X.values[i, :], training_transformed_Y.values[i]\n",
    "        t += 1\n",
    "        eta_t = 1.0/(t * Lambda)\n",
    "\n",
    "        X_i_pred = np.dot(X_i, beta) + alpha\n",
    "        if y_i * X_i_pred < 1.0:\n",
    "            beta = beta - eta_t *(Lambda * beta - y_i * X_i)\n",
    "            alpha = alpha - eta_t * (-y_i)\n",
    "        else:\n",
    "            beta = beta - eta_t * Lambda * beta\n",
    "            alpha = alpha - eta_t * 0        \n",
    "    # update prediction\n",
    "    training_Y_pred = np.dot(training_X.values, beta) + alpha\n",
    "\n",
    "    # compute loss function\n",
    "    loss = 0.5 * Lambda * np.linalg.norm(beta) + np.mean(np.maximum(np.zeros(n_training_samples), 1.0 - training_transformed_Y.values * training_Y_pred))\n",
    "\n",
    "    print(f\"Loss = {loss}, at Epoch = {epoch}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training data\n",
    "y_train_raw_prediction = np.dot(training_X.values, beta) + alpha\n",
    "y_train_prediction = np.zeros(n_training_samples)\n",
    "for i in range(n_training_samples):\n",
    "    if y_train_raw_prediction[i] < 0:\n",
    "        y_train_prediction[i] = -1\n",
    "    else:\n",
    "        y_train_prediction[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test data\n",
    "y_test_raw_prediction = np.dot(test_X.values, beta) + alpha\n",
    "y_test_prediction = np.zeros(n_test_samples)\n",
    "for i in range(n_test_samples):\n",
    "    if y_test_raw_prediction[i] < 0:\n",
    "        y_test_prediction[i] = -1\n",
    "    else:\n",
    "        y_test_prediction[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_confusion_matrix(y_predict, y_measurement):\n",
    "    TP = sum((y_predict==1) & (y_measurement==1))\n",
    "    TN = sum((y_predict==-1) & (y_measurement==-1))\n",
    "    FP = sum((y_predict==1) & (y_measurement==-1))\n",
    "    FN = sum((y_predict==-1) & (y_measurement==1))\n",
    "\n",
    "    print(f\"Error rate = {(FP + FN)/(TP + TN + FP + FN) * 100} %\")\n",
    "    \n",
    "    return np.array([[TP, FP],\n",
    "                    [FN, TN]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate = 0.0 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[100,   0],\n",
       "       [  0, 100]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_confusion_matrix(y_train_prediction, training_transformed_Y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate = 7.333333333333333 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[265,   9],\n",
       "       [ 35, 291]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_confusion_matrix(y_test_prediction, test_transformed_Y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
