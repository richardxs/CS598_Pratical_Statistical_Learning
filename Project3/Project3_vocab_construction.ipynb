{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oDn4CJ50He51"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "GnGmLq0FHe53"
   },
   "outputs": [],
   "source": [
    "prject_folder = \"proj3_data/split_\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q65sDhfgHe53"
   },
   "source": [
    "### Step 1: Load and preprocess training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(os.path.join(prject_folder+str(1), \"train.tsv\"), sep='\\t', header=0, dtype=str)\n",
    "df_test = pd.read_csv(os.path.join(prject_folder+str(1), \"test.tsv\"), sep='\\t', header=0, dtype=str)\n",
    "df_test['sentiment'] = pd.read_csv(os.path.join(prject_folder+str(1), \"test_y.tsv\"), sep='\\t', header=0, dtype=str)['sentiment']\n",
    "df_train = pd.concat([df_train, df_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "RFZvrj6lHe54"
   },
   "outputs": [],
   "source": [
    "df_train['review'] = df_train['review'].str.replace('<.*?>', '', regex=True)\n",
    "df_train['review'] = df_train['review'].str.replace('&lt;.*?&gt;', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "0xHc675vHe54"
   },
   "outputs": [],
   "source": [
    "positive_indices = df_train[df_train['sentiment']=='1'].index.values\n",
    "negative_indices = df_train[df_train['sentiment']=='0'].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "pmdHcm3HHe54"
   },
   "outputs": [],
   "source": [
    "num_pos = len(positive_indices)\n",
    "num_neg = len(negative_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7jusYEE6He54"
   },
   "source": [
    "### Step 2: Construct DT (DocumentTerm) matrix (maximum 4-grams)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "_8_IKCJxHe55"
   },
   "outputs": [],
   "source": [
    "stop_words = [\"i\", \"me\", \"my\", \"myself\",\n",
    "               \"we\", \"our\", \"ours\", \"ourselves\",\n",
    "               \"you\", \"your\", \"yours\",\n",
    "               \"their\", \"they\", \"his\", \"her\",\n",
    "               \"she\", \"he\", \"a\", \"an\", \"and\",\n",
    "               \"is\", \"was\", \"are\", \"were\",\n",
    "               \"him\", \"himself\", \"has\", \"have\",\n",
    "               \"it\", \"its\", \"the\", \"us\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    preprocessor=lambda x: x.lower(),  # Convert to lowercase\n",
    "    stop_words=stop_words,             # Remove stop words\n",
    "    ngram_range=(1, 4),               # Use 1- to 4-grams\n",
    "    min_df=0.001,                        # Minimum document frequency\n",
    "    max_df=0.5,                       # Maximum document frequency\n",
    "    token_pattern=r\"\\b[\\w+\\|']+\\b\" # Use word tokenizer: See Ethan's comment below\n",
    ")\n",
    "\n",
    "dtm_train = vectorizer.fit_transform(df_train['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "MnYYBY-GHe55"
   },
   "outputs": [],
   "source": [
    "df_features_names = pd.DataFrame(vectorizer.get_feature_names_out(), columns=['feature_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tc8K6odQHe55"
   },
   "source": [
    "## Step 3: Two sample t-test calculation\n",
    "Filter the vocabulary to include only terms I could readily interpret. I employed a straightforward screening method: the two-sample t-test. This test compares one-dimensional observations from two groups, denoted as:$X_1, X_2, ..., X_m$, $Y_1, Y_2, ..., Y_n$\n",
    "\n",
    "The goal is to determine whether the X population and the Y population share the same mean. The two-sample t-statistic is computed as:\n",
    "$$\n",
    "\\frac{\\bar X - \\bar Y}{\\sqrt{\\frac{\\sigma_{X}^2}{m} + \\frac{\\sigma_{Y}^2}{n}}}\n",
    "$$\n",
    "where $\\sigma_{X}^2$, $\\sigma_{Y}^2$ denote the sample variance of $X$ and $Y$\n",
    "\n",
    "> Suppose we have $m$ positive reviews and $n$ negative reviews. For a given word, $X_i$'s represent the measurements associated with each of the positive reviews, and $Y_j$'s represent the measurements corresponding to each of the negative reviews. The goal is to assess the significance of differences in measurements between the two sentiment groups for each word via t-test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_train_squared = dtm_train.copy()\n",
    "dtm_train_squared.data **= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_mean = dtm_train[positive_indices].mean(axis=0)\n",
    "pos_var = dtm_train_squared[positive_indices].mean(axis=0) - np.square(dtm_train[positive_indices].mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_mean = dtm_train[negative_indices].mean(axis=0)\n",
    "neg_var = dtm_train_squared[negative_indices].mean(axis=0) - np.square(dtm_train[negative_indices].mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_statistics_array = (pos_mean - neg_mean) / np.sqrt(pos_var / num_pos + neg_var / num_neg)\n",
    "t_statistics_array = np.squeeze(np.asarray(t_statistics_array))\n",
    "t_statistics_descending_array = t_statistics_array.argsort()[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QlzF03IMHe55"
   },
   "source": [
    "> Top 50 **positive words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rgHxraZjHe55",
    "outputId": "743258c0-17bd-492f-a6de-e6a3b0a53fc9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['great', 'excellent', 'best', 'of best', 'love', 'wonderful',\n",
       "       'one of best', 'brilliant', 'well', 'perfect', 'amazing', 'life',\n",
       "       'loved', 'favorite', 'beautiful', 'also', 'very', 'must see',\n",
       "       'one of', 'superb', 'highly', 'performance', 'wonderfully',\n",
       "       'fantastic', 'both', 'enjoyed', 'very well', 'performances',\n",
       "       'years', 'this great', 'always', 'gives', 'world', 'definitely',\n",
       "       'moving', 'especially', 'touching', 'story of', 'young',\n",
       "       'powerful', 'strong', 'great film', 'job', 'love this',\n",
       "       'well worth', 'perfectly', 'enjoy', 'gem', 'greatest', 'well as'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features_names['feature_names'].values[t_statistics_descending_array[:50]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uiAEkTAkHe55"
   },
   "source": [
    "> Top 50 **Negative words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xPbwMxg0He56",
    "outputId": "04a09b01-14f7-482f-d726-a2059bfd5711"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['badly', 'cheap', 'worst movie', 'waste time', 'supposed to be',\n",
       "       'thing', 'only', 'pointless', 'or', 'instead', 'pathetic', \"don't\",\n",
       "       'not even', 'least', 'laughable', '1', 'oh', 'script', 'lame',\n",
       "       'supposed to', 'poorly', 'crap', 'why', 'money', 'ridiculous',\n",
       "       'waste of time', 'supposed', 'at all', 'stupid', 'acting', 'avoid',\n",
       "       'so bad', 'horrible', 'one of worst', 'plot', 'just', 'even',\n",
       "       'worse', 'minutes', 'of worst', 'boring', 'waste of', 'poor', 'no',\n",
       "       'nothing', 'terrible', 'awful', 'waste', 'worst', 'bad'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features_names['feature_names'].values[t_statistics_descending_array[-50:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0DKOi5vHe56"
   },
   "source": [
    "> Select 2000 words with top absolute t_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "U3svgsUqHe56"
   },
   "outputs": [],
   "source": [
    "absolute_t_statistics_array = np.abs(t_statistics_array)\n",
    "absolute_t_stastics_descending_array_index = absolute_t_statistics_array.argsort()[::-1]\n",
    "selected_idx = absolute_t_stastics_descending_array_index[:2000]\n",
    "selected_vocab_2k = df_features_names['feature_names'].values[list(selected_idx)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XP38TE-_He56"
   },
   "source": [
    "### Step 4: use Lasso (with logistic regression) to trim the vocabulary size to 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "1ZUPkHOZHe56"
   },
   "outputs": [],
   "source": [
    "# Construct new DT matrix\n",
    "vectorizer2k = CountVectorizer(\n",
    "    ngram_range=(1, 2),               # Use 1- to 4-grams\n",
    "    vocabulary = selected_vocab_2k    # use 2000 vocabulary from the previous step\n",
    ")\n",
    "\n",
    "dtm_train2k = vectorizer2k.fit_transform(df_train['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Fit a lasso model 50 times, each time on 60% of data and record the features selected by Lasso. Save the top 1000 most frequently selected words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_count = df_train.shape[0]\n",
    "training_index = np.arange(training_count)\n",
    "selection_freq = np.zeros(dtm_train2k.shape[1])\n",
    "for i in range(50):\n",
    "    np.random.seed(i)\n",
    "    np.random.shuffle(training_index)\n",
    "    curr_idx = training_index[:int(training_count*0.6)]\n",
    "    curr_data = dtm_train2k[curr_idx]\n",
    "    lasso_model = LogisticRegression(penalty='l1', solver='liblinear', C = 0.3)\n",
    "    lasso_model.fit(X=curr_data, y=df_train['sentiment'].iloc[curr_idx])\n",
    "    selection_freq += np.squeeze(lasso_model.coef_) != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bad' 'quite' 'seemed' 'even better' 'okay' 'not recommend'\n",
      " 'very disappointed' 'instead of' 'tries' 'may not' 'unwatchable' 'tells'\n",
      " 'become' 'nowhere' 'supposedly' 'fascinating' 'appreciated' 'funniest'\n",
      " 'much better' 'any of' 'light' 'might' 'dvd' 'mildly' 'exceptional'\n",
      " 'makes' 'ages' 'see' 'nonsense' 'cardboard' 'superbly' 'suppose'\n",
      " 'intense' 'although' 'stay away' 'meets' 'recommend this' 'frankly'\n",
      " 'really good' 'fast paced' 'gritty' 'endless' 'lacks' 'chilling'\n",
      " 'clichés' 'sequel' 'filmmakers' 'surprised' 'atmosphere' 'human'\n",
      " 'pretentious' 'enjoyable' 'refreshing' 'camera' 'uninteresting'\n",
      " 'make movie' 'appalling' 'be missed' 'embarrassed' 'sucked' 'that bad'\n",
      " 'painful' 'movie just' 'first time' 'falls' 'forgettable' 'satisfying'\n",
      " 'saving' 'delightful' 'clichéd' 'hoping' 'works' 'completely' 'rubbish'\n",
      " 'journey' 'uninspired' 'way too' 'failed' 'entertaining' 'story'\n",
      " 'with this' 'first saw' 'only good' 'wonder' 'subtle' 'quiet'\n",
      " 'unconvincing' 'just not' 'what makes' 'dialogue' 'for everyone'\n",
      " 'premise' 'haunting' 'sometimes' 'fine' 'rare' 'hooked' 'recommend'\n",
      " 'french' 'tears' 'but this' 'solid' 'skip this' 'unforgettable'\n",
      " 'impressed' 'unlike' 'fun' 'victims' 'wasting' 'just as' 'well done'\n",
      " 'as if' 'stinker' 'wrong' 'same time' 'trite' 'touched' 'emotions'\n",
      " 'writers' 'liked' 'beauty' 'future' 'delivers' 'first rate' 'actors'\n",
      " 'dimensional' 'noir' 'that would' 'stands' 'lacked' 'looking'\n",
      " 'extraordinary' 'cover' 'well made' 'neither' 'incoherent' 'ride' 'doing'\n",
      " 'ugly' 'two hours' 'below average' 'subtitles' 'director' 'basically'\n",
      " 'would love' 'nevertheless' 'shallow' 'anyone who' 'wanted to' 'blah'\n",
      " 'watchable' 'for this' 'deals with' 'carries' 'value' 'paper' 'no one'\n",
      " 'effective' 'boredom' 'episodes' 'ways' 'pace' 'not very' 'be comedy'\n",
      " 'ed wood' 'how not' 'screaming' 'genius' 'get' 'had high' 'stilted'\n",
      " 'truly' 'to believe' 'traditional' 'world of' 'how good' 'interesting'\n",
      " 'matthau' 'of war' 'confusing' 'mst3k' 'to all' 'easy' 'reality'\n",
      " 'fast forward' 'not make' 'bore' 'thin' 'letdown' 'disjointed'\n",
      " 'realistic' 'at first' 'little to' 'olds' 'late' 'treat' 'downhill'\n",
      " 'too many' 'save this' 'at times' 'want to' 'ever' 'say' 'saw this'\n",
      " 'care about' 'not enough' 'thanks' 'definitely worth' 'turkey' 'innocent'\n",
      " 'drivel' 'of talent' 'alright' 'together' 'obnoxious' 'irritating' 'most'\n",
      " 'potential' 'bit' 'compelling' 'surprisingly' 'work with' 'pleased'\n",
      " 'seems' 'very entertaining' 'cliché' 'tripe' 'effort' 'paint' 'disaster'\n",
      " 'great acting' 'obvious' 'original' 'miscast' 'believable'\n",
      " 'mystery science' 'chance' 'not much' 'experience' 'yawn' 'up' 'dull'\n",
      " 'predictable' 'definitely' 'world' 'off' 'gives' 'garbage' 'reason'\n",
      " 'mess' 'always' 'unless' 'fails' 'to be' 'any' 'to make' 'shows' 'half'\n",
      " 'boll' 'years' 'could' 'performances' 'very well' 'save' 'avoid this'\n",
      " 'enjoyed' 'gorgeous' 'annoying' 'deserves' 'both' 'worst movies'\n",
      " 'especially' 'touching' 'powerful' 'heart' 'so' 'dumb' 'weak' 'different'\n",
      " 'than this' 'sorry' 'greatest' 'gem' 'enjoy' 'perfectly' 'like' 'feel'\n",
      " 'unfortunately' 'how bad' 'well worth' 'favourite' 'job' 'anything'\n",
      " 'seagal' 'strong' 'not funny' 'bravo' 'lousy' 'bother' 'redeeming'\n",
      " 'would' 'minutes of' 'wasted' 'fantastic' 'perfect' 'stupid' 'read'\n",
      " 'acting' 'avoid' 'so bad' 'brilliant' 'wonderful' 'horrible' 'plot'\n",
      " 'love' 'inventive' 'just' 'even' 'nicely' 'worse' 'minutes' 'boring'\n",
      " 'poor' 'best' 'no' 'nothing' 'terrible' 'excellent' 'awful' 'waste'\n",
      " 'great' 'worst' 'at all' 'amazing' 'wonderfully' 'supposed' 'badly'\n",
      " 'cheap' 'worst movie' 'only' 'pointless' 'superb' 'instead' 'one of'\n",
      " 'pathetic' 'not even' 'least' 'laughable' 'must see' 'oh' 'very' 'script'\n",
      " 'also' 'lame' 'segal' 'beautiful' 'favorite' 'loved' 'poorly' 'why'\n",
      " 'money' 'life' 'ridiculous' 'ok' 'love this' 'worth seeing' 'do not'\n",
      " 'sit through' 'brilliantly' 'sucks' 'flat' 'music' 'even that' 'stunning'\n",
      " 'unexpected' 'should' 'unfunny' 'hilarious' 'attempt at' 'problem'\n",
      " 'complex' 'apparently' 'whatsoever' 'this thing' 'highly recommended'\n",
      " 'dreadful' 'very bad' 'silly' 'highly recommend' 'underrated'\n",
      " 'amateurish' 'either' '10 10' 'still' 'finest' 'none of' 'true'\n",
      " 'remarkable' 'well written' 'each' 'material' 'thing about' 'unique'\n",
      " 'enjoyed this' 'loved this' 'only thing' 'season' 'sadly' 'bland'\n",
      " 'embarrassing' 'great job' 'not worth' 'looked' 'trash' 'this piece'\n",
      " 'not good' 'this crap' 'low' 'freedom' 'or something' 'look like'\n",
      " 'disappointing' 'unbelievable' 'disappointment' 'awesome' 'no sense'\n",
      " 'series' 'looks' 'excuse' 'mediocre' 'recommended' 'idea' 'available'\n",
      " 'outstanding' 'trying' 'effects' 'today' 'barely' 'at best' 'terrific'\n",
      " 'sweet' 'except' 'great movie' 'captures' 'wooden' 'guess' 'masterpiece'\n",
      " 'will' '10 out' 'crappy' 'classic' 'if this' 'tedious' 'someone' 'own'\n",
      " 'lives' 'bored' 'beautifully' 'on dvd' 'very good' 'simple' '80'\n",
      " 'very funny' 'incredible' 'piece of' 'minutes into' 'below' 'concept'\n",
      " 'present' 'played' 'later' 'well' 'crap' 'surprisingly good' 'horribly'\n",
      " 'city' 'interesting but' 'junk' 'going for' 'unrealistic' 'cash in'\n",
      " 'sub par' 'but still' 'edge' 'touches' 'small' 'moving' 'grade' 'often'\n",
      " 'attempt to' 'marvelous' 'society' 'who' 'honestly' 'sex' 'rent'\n",
      " 'great film' 'sharp' 'so called' 'excellently' 'lack of' 'seconds'\n",
      " 'worthless' 'man' 'useless' 'performance as' 'on this' 'good as' 'new'\n",
      " 'been' 'miss this' 'once' 'sleep' 'comedy' 'scientist' 'or' 'highly'\n",
      " 'flawless' 'edge of' 'make' 'absolutely no' 'warm' '90 minutes' 'frank'\n",
      " 'paid' 'unintentional' 'ludicrous' 'family' 'god' 'relationship'\n",
      " 'rip off' 'stupidity' 'adds' 'joke' 'excellent film' 'tolerable' 'brings'\n",
      " 'thumbs up' 'every day' 'certainly' 'reason for' 'about as' 'terribly'\n",
      " 'patient' 'absurd' 'harry' 'sisters' 'glued' 'not seen' 'soundtrack'\n",
      " 'horror' 'far' 'only reason' 'pretty bad' 'main problem' 'going' 'many'\n",
      " 'hour' 'happy' 'yet' 'bad but' 'insulting' 'sake' 'thought provoking'\n",
      " 'david' 'mainstream' 'day' 'as good' 'brutal' 'by' 'just great' 'side of'\n",
      " 'remotely' 'so well' 'surprising' 'steals' 'history' 'rented' 'kills'\n",
      " 'mother' 'too much' 'story of' 'then' 'believe that' 'those who'\n",
      " 'more like' 'generous' 'friendship' 'snakes' 'barbara' 'cannibal'\n",
      " 'decent' 'ashamed' 'lost interest' 'otherwise' 'timing' 'courage'\n",
      " 'just plain' 'not be' 'be funny' 'sort' 'personal' 'naked' 'lemmon'\n",
      " 'world war' 'best film' 'few laughs' 'romantic' 'others' 'those of'\n",
      " 'familiar' 'chick' 'stinks' 'good job' 'among' 'even get' 'dream' 'skip'\n",
      " 'thrown' 'did' 'hardly' 'but why' 'through this' 'oscar' 'some kind'\n",
      " 'won' 'cast' 'war' 'project' 'ultimate' 'past' 'far too' 'really bad'\n",
      " 'watching' 'bottom of' 'better' 'also good' 'running' 'point' 'in love'\n",
      " 'killed' 'rest' 'made' 'memorable' 'maybe' 'moved' 'watching this'\n",
      " 'helps' 'vhs' 'movie ever' 'nonetheless' 'some' 'provides' 'good too'\n",
      " 'zombies' 'overacting' 'fabulous' 'come on' '10 minutes' 'repetitive'\n",
      " 'nothing to' 'this turkey' 'why do' 'too long' 'this mess' 'landscape'\n",
      " 'pretty' 'holds' 'sit back' 'why did' 'score' 'please' 'else' 'times'\n",
      " 'insult' 'there' 'complete' 'sense' 'write' 'if' 'tried' 'devito'\n",
      " 'stick to' 'feelings' 'bad movie' 'enough' 'tragedy' 'say about' 'out of'\n",
      " 'wearing' 'this movie' 'like bad' 'mean' 'thing' 'all for' 'movie no'\n",
      " 'something better' 'deserve' 'absolutely nothing' 'somehow' 'wrenching'\n",
      " 'jean' 'better to' 'natural' 'important' 'should never' 'because'\n",
      " 'pleasantly surprised' 'cgi' 'perfection' 'to do' 'favorites' 'no real'\n",
      " 'just bad' 'tale' 'disgusting' 'deeper' 'walked' 'notch' 'something'\n",
      " 'charming' 'strength' 'rupert' 'finds' 'feeling of' 'worst film' 'sent'\n",
      " 'no plot' 'turd' 'waste of' 'relate' 'entire movie' 'utter' 'had'\n",
      " 'better movie' 'would be' 'seen this' 'what could' 'monster' 'viewing'\n",
      " 'shines' 'happiness' 'minor' 'much' 'master' 'unnecessary' 'delight'\n",
      " 'gore' 'spent' 'makers' 'perfect as' 'to write' 'atrocious' 'documentary'\n",
      " 'shoot' 'father' 'role' 'inept' 'visual' 'mistake' 'jokes' 'willie'\n",
      " 'walking around' 'unappealing' 'little else' 'idiotic' 'zombie' 'song'\n",
      " 'cut' 'kill' 'up with' 'top notch' 'unfortunately this' 'unusual'\n",
      " 'understated' 'like watching' 'definitely recommend' 'brought'\n",
      " 'producers' 'too' 'some good' 'insult to' 'riveting' 'spend'\n",
      " 'worst films' 'killer' 'by what' 'living' 'early' 'bad this' 'joy to'\n",
      " 'jimmy' 'stick' 'shows how' 'themes' 'this great' 'budget' 'neatly'\n",
      " 'fell in' 'but even' 'in turn' 'or funny' 'film for' 'so real'\n",
      " 'overlooked' 'released' 'asleep' 'looked like' 'all time' 'that this'\n",
      " 'dire' 'to say' 'shots' 'heartbreaking' 'random' 'to avoid' 'do'\n",
      " 'special effects' 'played by' 'of crap' 'nudity' 'for those' 'ugh'\n",
      " 'success' 'dark' 'days' 'blame' 'walk' 'want' 'slightest' 'whole thing'\n",
      " 'given' 'must' 'bad movies' '45' 'easy to' 'magnificent' 'recently'\n",
      " 'contrived' 'trying to' 'unbearable' 'actually' 'bunch of' 'try' 'lovely'\n",
      " 'very much' 'just too' 'grand' 'continuity' 'with great' 'favorite of'\n",
      " 'rounded' 'straight' 'this bad' 'this just' 'horrid' 'for best' 'young'\n",
      " 'well acted' 'looks like' 'award' 'sure to' 'immensely' 'moments'\n",
      " 'portrayal' 'throughout' 'favorite movies' 'scenes' 'very enjoyable'\n",
      " 'guy' 'bad acting' 'whole movie' 'joy' 'crime' 'best movie' 'walked out'\n",
      " 'hell' 'really disappointed' 'for anyone' 'franchise' 'low budget'\n",
      " 'good but' 'toilet' 'saying' 'animated' 'movie only' 'cinema' 'gripping'\n",
      " 'nothing more' 'act' 'obviously' 'tends' 'charlie' 'forced' 'favor'\n",
      " 'stewart' 'very moving' 'get better' 'best of' 'sit' 'as well' 'insipid'\n",
      " 'adventure' 'with bad' 'this could' 'meaningless' 'explained' 'between'\n",
      " 'nor' 'costs' 'inspired' 'james' 'john' 'or so' 'this story' 'plays'\n",
      " 'only saving' 'no chemistry' 'roles' 'yelling' 'shoddy' 'watch this'\n",
      " 'ridiculously' 'bunch' 'musical' 'movie had' 'approach' 'whole'\n",
      " 'contrast' 'art' 'of love' 'international film' 'sounds like' 'at time'\n",
      " 'james stewart' 'lines' 'of actors' 'porno' 'humour' 'performances from'\n",
      " 'of bad' 'making' 'role as' 'achievement' 'unfolds' 'seriously'\n",
      " 'affection' 'rented this' 'money on' 'entertaining film' 'including'\n",
      " 'also shows' 'with no' 'inspiring' 'specially' 'movie could' 'walking'\n",
      " 'or even' 'best films' 'but no' 'famous' 'sorry but' 'fisted' 'fake'\n",
      " 'arthur' 'drama' 'existent' 'animation' 'be disappointed' 'captured'\n",
      " 'davis' 'does nothing' 'renting' 'not watch' 'everyone should' 'walt'\n",
      " 'total' 'name' 'moronic' 'throw' 'as to' 'decided' 'macy' 'performance'\n",
      " 'incomprehensible' 'forwarding' 'most boring' 'live' 'creates' 'might be'\n",
      " 'relaxed' 'dialog' 'cheesy' 'sitting through' 'richard' 'suck' 'memories'\n",
      " 'dud' 'kind of' 'at least']\n"
     ]
    }
   ],
   "source": [
    "idx_by_freq = np.argsort(selection_freq)[::-1]\n",
    "top_vocab = selected_vocab_2k[idx_by_freq[:1000]]\n",
    "print(top_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = open(r'myvocab.txt', 'w')\n",
    "for item in top_vocab:\n",
    "    fp.write(\"%s\\n\" % item)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
