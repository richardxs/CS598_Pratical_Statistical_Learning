{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oDn4CJ50He51"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, LassoCV, Lasso, RidgeClassifier, RidgeClassifierCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "GnGmLq0FHe53"
   },
   "outputs": [],
   "source": [
    "prject_folder = \"proj3_data/split_\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q65sDhfgHe53"
   },
   "source": [
    "### Step 1: Load the training data and clean the html tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "K0xNhNNMaVbW"
   },
   "outputs": [],
   "source": [
    "df_train = pd.concat([pd.read_csv(os.path.join(prject_folder+str(fold), \"train.tsv\"), sep='\\t', header=0, dtype=str) for fold in range(1, 6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YZSiX63RYt2Q",
    "outputId": "86599550-09e2-405d-d140-008213c5556b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125000, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "RFZvrj6lHe54"
   },
   "outputs": [],
   "source": [
    "df_train['review'] = df_train['review'].str.replace('&lt;.*?&gt;', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "0xHc675vHe54"
   },
   "outputs": [],
   "source": [
    "positive_indices = df_train[df_train['sentiment']=='1'].index.values\n",
    "negative_indices = df_train[df_train['sentiment']=='0'].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "pmdHcm3HHe54"
   },
   "outputs": [],
   "source": [
    "num_pos = len(positive_indices)\n",
    "num_neg = len(negative_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7jusYEE6He54"
   },
   "source": [
    "### Step 2: Construct DT (DocumentTerm) matrix (maximum 4-grams).\n",
    "> The default vocabulary size (i.e., # of columns of dtm_train) is more than 30,000, bigger than the sample size n = 25,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "_8_IKCJxHe55"
   },
   "outputs": [],
   "source": [
    "stop_words = [\"i\", \"me\", \"my\", \"myself\",\n",
    "               \"we\", \"our\", \"ours\", \"ourselves\",\n",
    "               \"you\", \"your\", \"yours\",\n",
    "               \"their\", \"they\", \"his\", \"her\",\n",
    "               \"she\", \"he\", \"a\", \"an\", \"and\",\n",
    "               \"is\", \"was\", \"are\", \"were\",\n",
    "               \"him\", \"himself\", \"has\", \"have\",\n",
    "               \"it\", \"its\", \"the\", \"us\", \"br\"]\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    preprocessor=lambda x: x.lower(),  # Convert to lowercase\n",
    "    stop_words=stop_words,             # Remove stop words\n",
    "    ngram_range=(1, 4),               # Use 1- to 4-grams\n",
    "    min_df=0.001,                        # Minimum term frequency\n",
    "    max_df=0.5,                       # Maximum document frequency\n",
    "    token_pattern=r\"\\b[\\w+\\|']+\\b\" # Use word tokenizer: See Ethan's comment below\n",
    ")\n",
    "\n",
    "dtm_train = vectorizer.fit_transform(df_train['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125000, 31536)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "MnYYBY-GHe55"
   },
   "outputs": [],
   "source": [
    "df_features_names = pd.DataFrame(vectorizer.get_feature_names_out(), columns=['feature_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tc8K6odQHe55"
   },
   "source": [
    "## Step 3: Two sample t-test calculation\n",
    "Filter the vocabulary to include only terms I could readily interpret. I employed a straightforward screening method: the two-sample t-test. This test compares one-dimensional observations from two groups, denoted as:$X_1, X_2, ..., X_m$, $Y_1, Y_2, ..., Y_n$\n",
    "\n",
    "The goal is to determine whether the X population and the Y population share the same mean. The two-sample t-statistic is computed as:\n",
    "$$\n",
    "\\frac{\\bar X - \\bar Y}{\\sqrt{\\frac{\\sigma_{X}^2}{m} + \\frac{\\sigma_{Y}^2}{n}}}\n",
    "$$\n",
    "where $\\sigma_{X}^2$, $\\sigma_{Y}^2$ denote the sample variance of $X$ and $Y$\n",
    "\n",
    "> Suppose we have $m$ positive reviews and $n$ negative reviews. For a given word, $X_i$'s represent the measurements associated with each of the positive reviews, and $Y_j$'s represent the measurements corresponding to each of the negative reviews. The goal is to assess the significance of differences in measurements between the two sentiment groups for each word via t-test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_train_squared = dtm_train.copy()\n",
    "dtm_train_squared.data **= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_mean = dtm_train[positive_indices].mean(axis=0)\n",
    "pos_var = dtm_train_squared[positive_indices].mean(axis=0) - np.square(dtm_train[positive_indices].mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_mean = dtm_train[negative_indices].mean(axis=0)\n",
    "neg_var = dtm_train_squared[negative_indices].mean(axis=0) - np.square(dtm_train[negative_indices].mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_statistics_array = (pos_mean - neg_mean) / np.sqrt(pos_var / num_pos + neg_var / num_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31536,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_statistics_array = np.squeeze(np.asarray(t_statistics_array))\n",
    "t_statistics_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "OTWKOJucHe55"
   },
   "outputs": [],
   "source": [
    "t_statistics_descending_array = t_statistics_array.argsort()[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QlzF03IMHe55"
   },
   "source": [
    "> Top 50 **positive words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rgHxraZjHe55",
    "outputId": "743258c0-17bd-492f-a6de-e6a3b0a53fc9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['great', 'excellent', 'wonderful', 'of best', 'one of best',\n",
       "       'best', 'amazing', 'superb', 'love', 'well', 'loved', 'must see',\n",
       "       'brilliant', 'one of', 'today', 'well worth', 'very', 'highly',\n",
       "       'perfect', 'enjoyed', 'outstanding', 'performance', 'wonderfully',\n",
       "       'young', 'also', 'life', 'this great', 'beautiful', 'strong',\n",
       "       'unique', 'performances', 'both', 'fantastic', 'own', 'definitely',\n",
       "       'beautifully', 'top notch', 'favorite', 'powerful', 'excellent as',\n",
       "       'sweet', 'touching', 'war', 'classic', 'well as', 'awesome',\n",
       "       'gives', 'story of', 'moving', 'best of'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features_names['feature_names'].values[t_statistics_descending_array[:50]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uiAEkTAkHe55"
   },
   "source": [
    "> Top **Negative words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xPbwMxg0He56",
    "outputId": "04a09b01-14f7-482f-d726-a2059bfd5711"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cheap', 'fails', 'instead', 'bad acting', 'any', 'waste time',\n",
       "       'badly', 'bother', 'annoying', 'unless', 'thing', 'wasted',\n",
       "       'avoid this', 'poorly', 'so bad', 'no sense', 'reason',\n",
       "       'supposed to be', 'crap', 'just', 'lame', 'dull', 'one of worst',\n",
       "       'waste of time', 'why', 'avoid', 'minutes', 'laughable', 'plot',\n",
       "       'ridiculous', 'of worst', 'not even', 'stupid', 'supposed to',\n",
       "       'horrible', 'supposed', 'even', 'at all', 'waste of', 'nothing',\n",
       "       'acting', 'worse', 'poor', 'boring', 'no', 'terrible', 'awful',\n",
       "       'waste', 'worst', 'bad'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features_names['feature_names'].values[t_statistics_descending_array[-50:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0DKOi5vHe56"
   },
   "source": [
    "> Select 2000 words with top absolute t_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "U3svgsUqHe56"
   },
   "outputs": [],
   "source": [
    "absolute_t_statistics_array = np.abs(t_statistics_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "St_iftIEHe56"
   },
   "outputs": [],
   "source": [
    "absolute_t_stastics_descending_array_index = absolute_t_statistics_array.argsort()[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "1vE3EpBZHe56"
   },
   "outputs": [],
   "source": [
    "selected_idx = absolute_t_stastics_descending_array_index[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Z-DiG2VbHe56"
   },
   "outputs": [],
   "source": [
    "selected_vocab_2k = df_features_names['feature_names'].values[list(selected_idx)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XP38TE-_He56"
   },
   "source": [
    "### Step 4: use Lasso (with logistic regression) to trim the vocabulary size iteratively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "oMUspNoiHe56"
   },
   "outputs": [],
   "source": [
    "lasso_model = LogisticRegression(penalty='l1', solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "1ZUPkHOZHe56"
   },
   "outputs": [],
   "source": [
    "vectorizer2k = CountVectorizer(\n",
    "    ngram_range=(1, 4),               # Use 1- to 4-grams\n",
    "    vocabulary = selected_vocab_2k\n",
    ")\n",
    "\n",
    "# vectorizer2k.fit(selected_vocab_2k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Kdjlt7R-He56"
   },
   "outputs": [],
   "source": [
    "dtm_train2k = vectorizer2k.transform(df_train['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rpfwo0SXJ1nN",
    "outputId": "ff051340-181c-4703-bff3-8193805f35a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125000, 2000)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm_train2k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_count = df_train.shape[0]\n",
    "training_index = np.arange(training_count)\n",
    "selection_freq = np.zeros(dtm_train2k.shape[1])\n",
    "for i in range(50):\n",
    "    np.random.seed(i)\n",
    "    np.random.shuffle(training_index)\n",
    "    curr_idx = training_index[:int(training_count*0.6)]\n",
    "    curr_data = dtm_train2k[curr_idx]\n",
    "    lasso_model = LogisticRegression(penalty='l1', solver='liblinear', C = 0.2)\n",
    "    lasso_model.fit(X=curr_data, y=df_train['sentiment'].iloc[curr_idx])\n",
    "    selection_freq += np.squeeze(lasso_model.coef_) != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_by_freq = np.argsort(selection_freq)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_vocab = selected_vocab_2k[idx_by_freq[:1000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['going for' 'caught' 'cash in' 'been' 'movie no' 'power of' 'insult to'\n",
      " 'may not' 'really good' 'explained' 'rip off' 'wasting' 'dvd' 'because'\n",
      " 'often' 'most' 'minutes into' 'useless' 'should' 'movie just' 'want'\n",
      " 'unintentional' 'recommended' 'frank' 'unbelievable' 'unconvincing'\n",
      " 'hoping' 'embarrassed' 'otherwise' 'shallow' 'nicely' 'series'\n",
      " 'worth seeing' 'affection' 'disjointed' 'loved this'\n",
      " 'highly recommend this' 'even get' 'prince' 'fears' 'fascinating'\n",
      " 'flawless' 'lost interest' 'deliciously' 'favorite movies' 'basically'\n",
      " 'revolting' 'not that' 'barbara' 'first saw' 'share' 'very good'\n",
      " 'soundtrack' 'insult' 'original' 'tells' 'impact' 'same time' 'miss this'\n",
      " 'movie ever' 'no chemistry' 'history' 'seconds' 'dreck' 'edge' 'timeless'\n",
      " 'country' 'way too' 'emotions' 'with this' 'played' 'past' 'drivel'\n",
      " 'very bad' 'not good' 'tense' 'this crap' 'brings' 'tries to be' 'gore'\n",
      " 'point' 'hilarious' 'warm' 'raw' 'plastic' 'many' 'tedious' 'by saying'\n",
      " 'disappointing' 'does nothing' 'tragic' 'shows' 'sometimes' 'just too'\n",
      " 'works' 'memorable' 'dialogue' '10 minutes' 'reality' 'complete'\n",
      " 'effects' 'job' 'sake' 'cliché' 'looking' 'chance' 'top' 'bin'\n",
      " 'forgettable' 'okay' 'gas' 'mention' 'blah' 'good too' 'inspiring'\n",
      " 'minor' 'through this' 'greatest' 'surreal' 'at time' 'well done'\n",
      " 'unrealistic' 'looked' 'absolutely nothing' 'that bad' 'gorgeous'\n",
      " 'for free' 'mst3k' 'trying to' 'embarrassing' 'writers' 'sit through'\n",
      " 'might' 'quite' 'boredom' 'different characters' 'uninspired' 'intense'\n",
      " 'made' 'writing' 'day' 'commercials' 'favorites' 'juliet' 'anything to'\n",
      " '3000' 'complex' 'fell' 'paper' 'unimaginative' 'believe' 'doing'\n",
      " 'walked' 'lacking' 'in way' 'accents' 'not in good' 'wow' 'specially'\n",
      " 'not very' 'treat' 'trying to be' '90 minutes' 'sex' 'excellent film'\n",
      " 'sucked' 'small' 'viewer' 'at first' 'for everyone' 'side' 'finest'\n",
      " 'very strong' 'make fun of' 'even worth' 'helps' 'kill' 'begins'\n",
      " 'adds to' 'twists' 'scary movie' 'academy' 'worthless' 'believable'\n",
      " 'or hate' 'continuity' 'entire' 'funny at all' 'meandering' 'than most'\n",
      " 'late' 'given this' 'not recommended' 'wonder' 'anyone interested in'\n",
      " 'moved' 'gripping' 'love to see' 'hooked' 'lovely' 'feel good' 'includes'\n",
      " 'abomination' 'would love' 'most boring' 'plodding' 'without any'\n",
      " 'cardboard' 'funniest' 'patient' 'layered' 'surprised' 'sense of'\n",
      " 'captures' 'killed' 'unwatchable' 'warmth' 'insulting' 'even that'\n",
      " 'inconsistent' 'this turkey' 'atmosphere' 'seems' 'quiet' 'ruined' 'sole'\n",
      " 'chilling' 'rest' 'potential' 'makes no' 'superbly' 'future'\n",
      " 'even close to' 'else' 'good as' 'painful' 'what makes' 'noir' 'absurd'\n",
      " 'nevertheless' 'brought' 'spent' 'supposedly' 'if had' 'obvious'\n",
      " 'very entertaining' '10 out' 'appreciate' 'wrong' 'feelings' 'actors'\n",
      " 'linda' 'going to' 'screenwriters' 'see' 'just enjoy' 'dollar'\n",
      " 'little to' 'below average' 'tasteless' 'some decent' 'storyline' 'demon'\n",
      " 'recommend' 'cleverly' 'later' 'paint' 'very different' 'problem' 'city'\n",
      " 'viewing' 'detail' 'not very good' 'excellently' 'be released' 'innocent'\n",
      " 'breath' 'spectacular' 'amateurish' 'too many' 'very disappointed'\n",
      " 'with children' 'nudity' 'very funny' 'watching this' 'beautifully'\n",
      " 'absolutely no' 'mess' 'script' 'nothing to' 'save' 'like' 'redeeming'\n",
      " 'sucks' 'actually' 'could' 'favorite' 'uninteresting' 'unfortunately'\n",
      " 'rubbish' 'so' 'perfectly' 'idea' 'unique' 'performances' 'sorry' 'both'\n",
      " 'fantastic' 'oh' 'how bad' 'pointless' 'money' 'own' 'definitely'\n",
      " 'worst movie' 'apparently' 'least' 'powerful' 'sweet' 'touching' 'beauty'\n",
      " 'great job' 'heart' 'definitely worth' 'very well' 'oscar' 'remarkable'\n",
      " 'impressed' 'war' 'lousy' 'dreadful' 'flat' 'minutes of' 'worst film'\n",
      " 'write' 'than this' 'worst movies' 'just bad' 'highly recommend'\n",
      " 'this piece' 'rare' 'gem' 'best of' 'just plain' 'or something' 'moving'\n",
      " 'ok' 'whatsoever' 'gives' 'awesome' 'classic' 'not worth' 'strong'\n",
      " 'garbage' 'beautiful' 'minutes' 'horrible' 'stupid' 'not even'\n",
      " 'ridiculous' 'plot' 'laughable' 'amazing' 'love' 'avoid' 'why' 'superb'\n",
      " 'dull' 'lame' 'just' 'supposed' 'best' 'even' 'at all' 'nothing' 'acting'\n",
      " 'worse' 'poor' 'boring' 'wonderful' 'no' 'terrible' 'awful' 'excellent'\n",
      " 'waste' 'great' 'worst' 'crap' 'reason' 'this great' 'outstanding' 'any'\n",
      " 'instead' 'fails' 'cheap' 'perfect' 'enjoyed' 'wonderfully' 'well'\n",
      " 'this movie' 'only' 'pathetic' 'also' 'life' 'predictable' 'highly'\n",
      " 'badly' 'very' 'bother' 'annoying' 'unless' 'well worth' 'today' 'wasted'\n",
      " 'one of' 'avoid this' 'brilliant' 'poorly' 'must see' 'so bad' 'loved'\n",
      " 'no sense' 'watching' 'completely' 'incomprehensible' 'instead of'\n",
      " 'stinks' 'marvelous' 'obnoxious' 'bland' 'lack of' 'half' 'only good'\n",
      " 'highly recommended' 'this mess' 'facing' 'traditional' 'this thing'\n",
      " 'bored' 'but this' 'remotely' 'save this' 'still' 'unfunny' 'portrayal'\n",
      " 'weak' 'atrocious' 'enjoy' 'looks like' 'impressive' 'any of' 'simple'\n",
      " 'zombies' 'horrid' 'skip this' 'yet' 'fast forward' 'trying' 'charming'\n",
      " 'not funny' 'someone' 'favourite' 'stay away' 'realistic' 'dumb' 'each'\n",
      " 'piece of' 'incredible' 'failed' 'look like' 'of war' 'silly' 'confusing'\n",
      " 'rented' 'will' 'human' 'friendship' 'some kind of' 'first time'\n",
      " 'haunting' 'idiotic' 'seemed' 'worst films' 'work of' 'subtitles'\n",
      " 'delight' 'stunning' 'theme' 'joke' 'incoherent' 'as if' 'much better'\n",
      " 'especially' 'masterpiece' 'family' 'trash' 'but no' 'either'\n",
      " 'underrated' 'attempt at' 'to be' 'available' 'really bad' 'always'\n",
      " 'love this' 'with great' 'to make' 'pretentious' 'perfection' 'no plot'\n",
      " 'disappointment' 'excuse' 'pretty bad' 'bad this' 'different' 'low'\n",
      " 'well written' 'would' 'relationship' 'off' 'extraordinary' 'brilliantly'\n",
      " 'mediocre' 'only reason' 'only thing' 'pretty' '10 10' 'true' 'anything'\n",
      " 'on dvd' 'none of' 'performance as' 'unforgettable'\n",
      " 'pleasantly surprised' 'world' 'if this' 'music' 'stinker' 'looks'\n",
      " 'great movie' 'wooden' 'guess' 'terrific' 'delightful' 'satisfying' 'say'\n",
      " 'journey' 'great film' 'lives' 'enjoyable' 'hour' 'years' 'fun' 'gritty'\n",
      " 'dialog' 'enjoyed this' 'mildly' 'solid' 'unfortunately this' 'subtle'\n",
      " 'very moving' 'clichés' 'except' 'tears' 'bad' 'one night' 'on this'\n",
      " 'complaint' 'this would' 'zero' 'any real' 'sees' 'compelling'\n",
      " 'something better' 'not seen' 'killer' 'most important' 'had to'\n",
      " 'this sequel' 'suppose' 'nowhere' 'be disappointed' 'bad but'\n",
      " 'can relate' 'mouse' 'for this' 'for anyone who' 'bad guys' 'episode'\n",
      " 'come on' 'steals' 'full' 'watchable' 'shines' 'unbelievably' 'not be'\n",
      " 'mistake' 'screaming' 'tried' 'that this movie' 'keeps' 'no idea what'\n",
      " 'ham' 'stock footage' 'this little' 'ludicrous' 'entertaining' 'concept'\n",
      " 'julia' 'what to expect' 'first' 'no story' 'clichéd' 'shelf' 'no one'\n",
      " 'plenty' 'care about' 'embarrassment' 'mood for' 'die' 'shameless'\n",
      " 'harry' 'turd' 'want to' 'stories' 'truly' 'devito' 'appreciated'\n",
      " 'present' 'effectively' 'negative reviews' 'grim' 'camera' 'well thought'\n",
      " 'captured' 'pre' 'last scene' 'downhill' 'only saving' 'more realistic'\n",
      " 'turkey' 'heartbreaking' 'looked like' 'episodes' 'paid' 'restored'\n",
      " 'been so much' 'dire' 'mature' 'elvis' 'isolation' 'must for' 'author'\n",
      " 'from outer space' 'great idea' 'so called' 'premise' 'not believable'\n",
      " 'look forward' 'total lack' 'enjoyable movie' 'must be' 'blatant'\n",
      " 'how good' 'miscast' 'dear' 'talks to' 'film ever' 'say about' 'hairy'\n",
      " 'easy to' 'but sadly' 'expertly' 'half hour' 'meets' 'just as' 'twisted'\n",
      " 'cried' 'overlooked' 'brutal' 'attempt to' 'irritating' 'influenced'\n",
      " 'ever' 'more like' 'new' 'reason to' 'obviously' 'whole movie' 'nature'\n",
      " 'does great' 'cheated' 'mean' 'in love' 'like some' 'this bad' 'mouth'\n",
      " 'monster' 'but also' 'low budget' 'thing that' 'beautiful film' 'america'\n",
      " 'voice over' 'this garbage' 'of time' 'adds' 'bad film'\n",
      " 'thoroughly enjoyed' 'bunch of' 'any of them' 'somebody' 'to do'\n",
      " 'stick to' 'of crap' 'had' 'deeper' 'worthwhile' 'very first'\n",
      " 'to come out' 'random' 'with this movie' 'blame' 'make' 'than watching'\n",
      " 'fest' 'falls' 'surprise' 'enough for' 'will keep' 'painfully' 'genuine'\n",
      " 'dark' 'won' 'be funny' 'here as' 'do not watch this' 'coherent'\n",
      " 'to sit through' 'waste of' 'out of' 'every second' 'stands' 'magically'\n",
      " 'stupidity' 'for long time' 'wondering' 'made this' 'too much'\n",
      " 'this piece of' 'young' 'just right' 'loves' 'of low budget' 'in paris'\n",
      " 'achievement' 'blond' 'all ages' 'then' 'anthony' 'make movie'\n",
      " 'hysterical' 'first saw this' 'tries' 'run around' 'special effects'\n",
      " 'this trash' 'bad movies' 'places' 'no real' 'with no' 'morgan' 'married'\n",
      " 'no one in' 'riveting' 'dud' 'fresh' 'this one just' 'magnificent'\n",
      " 'performances by' 'moronic' 'popular' 'chaplin' 'story' 'drags on'\n",
      " 'walk out' 'top notch' 'skip' 'of very' 'film that will' 'available to'\n",
      " 'tale' 'by' 'contrived' 'salt' 'joy to' 'infectious' 'feisty'\n",
      " 'interesting or' 'must see for' 'group of' 'do' 'limp' 'terrible acting'\n",
      " 'mother' 'zombie' 'whole' 'car' 'moments' 'role' 'part' 'sat through'\n",
      " 'of hollywood' 'want good' 'plain' 'if' 'with bad' 'holds' 'worse than'\n",
      " 'powerfully' 'then again' 'second rate' 'plot line' 'story of'\n",
      " 'no wonder' 'there no' 'bottom' 'add up' 'early' 'non existent'\n",
      " 'interact with' 'wonder why' 'stick' 'patrick' 'era' 'of most'\n",
      " 'supporting' 'movie could' 'there' 'unexciting' 'making' 'seem like'\n",
      " 'alan' 'none of that' 'love this movie' 'dumbest' 'whom' 'anthony quinn'\n",
      " 'sequel' 'avoided' 'any way' 'hollywood' 'plot that' 'sister' 'walks'\n",
      " 'growing up' 'no character' 'well as' 'destroyed' 'maybe' 'that gives'\n",
      " 'comes off' 'attempts at' 'between' 'hell' 'to say' 'brain' 'this just'\n",
      " 'provides' 'better' 'notch' 'something' 'ralph' 'can be' 'pile'\n",
      " 'money back' 'born in' 'huge disappointment' 'favor' 'of young' 'surf'\n",
      " 'creates' 'nothing to do' 'personal' 'haines' 'would be' 'or'\n",
      " 'only good thing' 'bad movie' 'love story' 'film with' 'memories'\n",
      " 'straight to' 'stewart' 'stiff' 'decent' 'say to' 'in role' 'slap'\n",
      " 'whole thing' 'pioneers' 'may not be' 'nothing good' 'kind' 'laughably'\n",
      " 'except to' 'essential' 'miserably' 'well acted' 'only redeeming'\n",
      " 'unoriginal' 'great as' 'understandable' 'minutes or so' 'blend of'\n",
      " 'five minutes' 'gradually' 'budget' 'wanted to make' 'scope' 'winning'\n",
      " 'adventure' 'sense at all' 'special mention' 'to slow' 'james' 'experts'\n",
      " 'some friends' 'underlying' 'also great' 'debut' 'ever seen' 'fabulous'\n",
      " 'some great' 'come up with' 'warned' 'or so' 'perfect as' 'guy' 'going'\n",
      " 'asleep' 'portrays' 'better than this' 'much' 'devoid of' 'thing'\n",
      " 'tenderness' 'very poor' 'wife' 'adorable' 'others in' 'money to'\n",
      " 'elegant' 'highly entertaining' 'around' 'can call' 'no apparent'\n",
      " 'some sort of' 'falls flat' 'well worth watching' 'polar' 'larger'\n",
      " 'walked out' 'cinematography by' 'world war' 'give' 'rewarding' 'visual'\n",
      " 'robert' 'begin with' 'unsettling' 'childhood' 'best films' 'so no'\n",
      " 'during' 'supposed to' 'of innocent' 'artist' 'be found in' 'on tv'\n",
      " 'idiot' 'something like' 'saying' 'plenty of' 'this dull' 'music by'\n",
      " 'no reason' 'only complaint' 'total' 'suck' 'even worse' '10 out of 10'\n",
      " 'use of' 'but at least' 'richard' 'gene' 'in new' 'excellent as'\n",
      " 'only thing that' 'supposed to be']\n"
     ]
    }
   ],
   "source": [
    "print(top_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = open(r'myvocab.txt', 'w')\n",
    "for item in top_vocab:\n",
    "    fp.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lc99YpeRHe5-"
   },
   "source": [
    "### Step 4: Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "byBFvSHJHe5-"
   },
   "outputs": [],
   "source": [
    "vectorizer2 = CountVectorizer(\n",
    "    ngram_range=(1, 4),               # Use 1- to 4-grams\n",
    "    vocabulary=top_vocab\n",
    ")\n",
    "\n",
    "# vectorizer2.fit(top_vocab)\n",
    "dtm_train2 = vectorizer2.transform(df_train['review'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "fFKG0OuyHe5-"
   },
   "outputs": [],
   "source": [
    "alphas = np.linspace(1, 10, 20)\n",
    "ridge_model = LogisticRegression(penalty='l2', solver='liblinear')\n",
    "ridge_clf = GridSearchCV(ridge_model, [{'C': alphas}], cv=10, refit=False, scoring='roc_auc')\n",
    "ridge_clf.fit(X=dtm_train2, y=df_train['sentiment'])\n",
    "best_alpha = ridge_clf.best_params_['C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "rH1Y3V4rHe5-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9631540356870454, 3.3684210526315788)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_clf.best_score_, best_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "OdibNhlsHe5-",
    "outputId": "84b0827e-2f43-4cef-fa35-0604479ba8b7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=3.3684210526315788, solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=3.3684210526315788, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=3.3684210526315788, solver='liblinear')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_ridge_model = LogisticRegression(penalty='l2', solver='liblinear', C = best_alpha)\n",
    "best_ridge_model.fit(X=dtm_train2, y=df_train['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whRvot2qHe5-"
   },
   "source": [
    "> The probability estimates correspond to the probability of the class with the greater label, i.e. estimator.classes_[1]\n",
    "and thus estimator.predict_proba(X, y)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_score_list = []\n",
    "for fold in range(1, 6):\n",
    "    df_test_x = pd.read_csv(os.path.join(prject_folder+str(fold), \"test.tsv\"), sep='\\t', header=0, dtype=str)\n",
    "    df_test_y = pd.read_csv(os.path.join(prject_folder+str(fold), \"test_y.tsv\"), sep='\\t', header=0, dtype=str)\n",
    "    df_test_x['review'] = df_test_x['review'].str.replace('&lt;.*?&gt;', ' ', regex=True)\n",
    "    dtm_test2= vectorizer2.transform(df_test_x['review'])\n",
    "    auc_score_list.append(roc_auc_score(df_test_y['sentiment'].values, best_ridge_model.predict_proba(dtm_test2)[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9608065563390693,\n",
       " 0.960927405211607,\n",
       " 0.9604661843656356,\n",
       " 0.9610337286615864,\n",
       " 0.9606003280894073]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_score_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JGow-0m5He5-"
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "myvocab = []\n",
    "with open(r'myvocab.txt', 'r') as fp:\n",
    "    for line in fp:\n",
    "        x = line[:-1]\n",
    "        myvocab.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    ngram_range=(1, 4),\n",
    "    vocabulary=myvocab\n",
    ")\n",
    "ridge_model = LogisticRegression(penalty='l2', solver='liblinear', C=0.3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_transform_data(data_path, vectorizer):\n",
    "    df_train = pd.read_csv(os.path.join(data_path, \"train.tsv\"), sep='\\t', header=0, dtype=str)\n",
    "    df_train['review'] = df_train['review'].str.replace('&lt;.*?&gt;', ' ', regex=True)\n",
    "    dtm_train = vectorizer.transform(df_train['review'])\n",
    "    train_y = df_train['sentiment']\n",
    "    df_test_x = pd.read_csv(os.path.join(data_path, \"test.tsv\"), sep='\\t', header=0, dtype=str)\n",
    "    df_test_x['review'] = df_test_x['review'].str.replace('&lt;.*?&gt;', ' ', regex=True)\n",
    "    dtm_test = vectorizer.transform(df_test_x['review'])\n",
    "    return dtm_train, train_y, dtm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "MZv9LJqJHe5-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1 training_time: 0.16851472854614258 pred_time: 0.003536224365234375 auc_score: 0.9524941463566892\n",
      "fold 2 training_time: 0.18445301055908203 pred_time: 0.001817941665649414 auc_score: 0.9527457485222572\n",
      "fold 3 training_time: 0.1652388572692871 pred_time: 0.0012230873107910156 auc_score: 0.9524304905599402\n",
      "fold 4 training_time: 0.16896891593933105 pred_time: 0.0016388893127441406 auc_score: 0.9529205970691821\n",
      "fold 5 training_time: 0.17342901229858398 pred_time: 0.002187013626098633 auc_score: 0.9526752215691583\n"
     ]
    }
   ],
   "source": [
    "auc_score_list = []\n",
    "for fold in range(1, 6):\n",
    "    data_path = prject_folder + str(fold)\n",
    "    dtm_train, train_y, dtm_test = read_and_transform_data(data_path, vectorizer)\n",
    "    time0 = time.time()\n",
    "    ridge_model.fit(X=dtm_train, y=train_y)\n",
    "    time1 = time.time()\n",
    "    y_proba = ridge_model.predict_proba(dtm_test)[:, 1]\n",
    "    time2 = time.time()\n",
    "    df_test_y = pd.read_csv(os.path.join(data_path, \"test_y.tsv\"), sep='\\t', header=0, dtype=str)\n",
    "    auc_score_list.append(roc_auc_score(df_test_y['sentiment'].values, y_proba))\n",
    "    print(\"fold\", fold, \"training_time:\", time1-time0, \"pred_time:\", time2-time1, \"auc_score:\", auc_score_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9524941463566892,\n",
       " 0.9527457485222572,\n",
       " 0.9524304905599402,\n",
       " 0.9529205970691821,\n",
       " 0.9526752215691583]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "MZv9LJqJHe5-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3593813663804626\n",
      "0.9525136984479116\n"
     ]
    }
   ],
   "source": [
    "for fold in range(1, 2):\n",
    "    data_path = prject_folder + str(fold)\n",
    "    dtm_train, train_y, dtm_test = read_and_transform_data(data_path, vectorizer)\n",
    "    \n",
    "    alphas = np.logspace(-2, 0, 10)\n",
    "    ridge_model = LogisticRegression(penalty='l2', solver='liblinear')\n",
    "    ridge_clf = GridSearchCV(ridge_model, [{'C': alphas}], cv=10, refit=False, scoring='roc_auc')\n",
    "    ridge_clf.fit(X=dtm_train, y=train_y)\n",
    "    best_alpha = ridge_clf.best_params_['C']\n",
    "    print(best_alpha)\n",
    "    \n",
    "    ridge_model = LogisticRegression(penalty='l2', solver='liblinear', C=best_alpha)\n",
    "    ridge_model.fit(X=dtm_train, y=train_y)\n",
    "    y_proba = ridge_model.predict_proba(dtm_test)[:, 1]\n",
    "    df_test_y = pd.read_csv(os.path.join(data_path, \"test_y.tsv\"), sep='\\t', header=0, dtype=str)\n",
    "    auc_score = roc_auc_score(df_test_y['sentiment'].values, y_proba)\n",
    "    print(auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually went over vocab list and deleted words like \"had\", \"then\", \"by\", \"do\", \"or\" and names"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
